# Hybrid TTTモデルの実験結果分析
## ゼミ発表資料

---

## 1. 実験概要

### 比較モデル
| モデル | 説明 | パラメータ数 |
|--------|------|-------------|
| **Base (TCFormer)** | ベースラインモデル | 74K-82K |
| **Hybrid Dynamic** | Entropy-driven TTT適応 | 78K-86K |

### 評価データセット
| データセット | クラス数 | 被験者数 | チャンネル数 | 特徴 |
|-------------|---------|---------|-------------|------|
| BCIC IV 2a | 4 | 9 | 22 | 標準的なMIデータ |
| BCIC IV 2b | 2 | 9 | 3 | 少チャンネル |
| HGD | 4 | 14 | 44 | 高密度・高品質 |

---

## 2. 実験結果

### 2.1 精度比較

| Dataset | Base | Hybrid Dynamic | 差分 |
|---------|------|----------------|------|
| **BCIC 2a** | **84.67%** ± 9.25 | 83.52% ± 5.52 | **-1.15%** |
| **BCIC 2b** | **82.67%** ± 6.73 | 80.76% ± 7.39 | **-1.91%** |
| **HGD** | **92.95%** ± 7.01 | 79.29% ± 14.61 | **-13.66%** |

### 2.2 訓練時間比較

| Dataset | Base | Hybrid | 短縮率 |
|---------|------|--------|--------|
| BCIC 2a | 43.3 min | 23.7 min | **1.8x** |
| BCIC 2b | 131.3 min | 12.1 min | **10.9x** |
| HGD | 212.2 min | 79.5 min | **2.7x** |

---

## 3. 被験者別詳細分析

### 3.1 HGDでの壊滅的低下

特に問題のある被験者:

| 被験者 | Base | Hybrid | 低下幅 |
|--------|------|--------|--------|
| S5 | **100.0%** | 75.0% | **-25.0%** |
| S7 | 86.9% | **51.3%** | **-35.6%** |
| S8 | 96.3% | 68.1% | **-28.1%** |
| S14 | 75.0% | 61.3% | **-13.8%** |

一方、維持・改善した被験者:
| 被験者 | Base | Hybrid | 差分 |
|--------|------|--------|------|
| S4 | 100.0% | 98.1% | -1.9% |
| S9 | 100.0% | 99.4% | -0.6% |
| S11 | 92.5% | 93.1% | **+0.6%** |
| S12 | 98.8% | 93.1% | -5.6% |

### 3.2 BCIC 2bでの被験者別差異

| 被験者 | Base | Hybrid | 差分 |
|--------|------|--------|------|
| S1 | 82.5% | 77.5% | -5.0% |
| S3 | 73.8% | 75.0% | **+1.3%** |
| S4 | **98.8%** | **100.0%** | **+1.2%** |
| S9 | 77.5% | 72.5% | -5.0% |

---

## 4. なぜうまくいかなかったのか？

### 4.1 原因1: Test-Time Training (TTT)の根本的問題

```
TTTの設計思想:
「テスト時にモデルを入力に適応させることで性能向上」

しかし現実には:
- 適応が「改善」ではなく「劣化」を引き起こすケースがある
- 特に、すでに高精度な予測に対して余計な調整を行う
```

**数式で説明:**
```
通常の推論: y = f(x; θ)
TTT:        y = f(x; θ + Δθ(x))

問題: Δθ(x)が必ずしも良い方向とは限らない
```

### 4.2 原因2: 訓練時とテスト時の挙動不一致

**Entropy Gatingの設計自体は正しい:**
```python
if entropy(prediction) > threshold:  # 予測が不確実
    α = high   # TTT適応を強める（正しい）
else:
    α = 0      # TTT適応をスキップ（正しい）
```

**しかし、設定に問題がある:**
```yaml
entropy_gating_in_train: False  # ← 訓練時は2-passを使わない！
```

**結果:**
| フェーズ | TTT制御方式 |
|---------|------------|
| 訓練時 | 静的α（常にTTT適用） |
| テスト時 | 動的α（エントロピーベース） |

→ **訓練とテストで挙動が異なり、学習した知識がテスト時に活かされない**

### 4.3 原因3: データセット特性との相性

| データセット | Base精度 | 信号品質 | TTTの影響 |
|-------------|---------|---------|----------|
| BCIC 2a | 84.7% | 中 | 小 (-1.2%) |
| BCIC 2b | 82.7% | 中 | 小 (-1.9%) |
| HGD | **92.9%** | **高** | **大 (-13.7%)** |

**観察:**
- Baseの精度が高いほど、TTTによる劣化が大きい
- HGDは信号品質が高く、Baseがほぼ最適解に到達している
- TTTが「すでに良い状態」を壊している

### 4.4 原因4: 学習不足（Early Stopping）

**訓練時間の比較:**
- Base: 長時間学習（十分に収束）
- Hybrid: 短時間で停止（Early Stopping）

**仮説:**
- Hybridモデルは「TTT機構の学習」も必要
- しかしEarly Stoppingで十分な学習前に停止
- TTT機構が正しく学習されていない

---

## 5. モデルアーキテクチャの問題点

### 5.1 TTT Layerの構造

```
入力 x
  ↓
[Base Encoder] → base_out
  ↓
[TTT Adapter]  → ttt_out (テスト時に重み更新)
  ↓
output = base_out + α × ttt_out
```

**問題:**
1. TTT Adapterの自己教師あり学習が不安定
2. αの制御が適切でない
3. TTTの学習率が大きすぎる可能性

### 5.2 Entropy Gatingの実装

```python
class EntropyGating:
    def forward(self, entropy):
        # 設計は正しい: 不確実(高エントロピー)な時にTTTを適用
        if entropy > threshold:
            α = sigmoid(w * (entropy - threshold) + b)  # TTT適応
        else:
            α = 0  # TTTをスキップ（Dead-zone保護）
```

**問題（設計ではなく設定）:**
- `entropy_gating_in_train: False` → 訓練/テストで挙動が異なる
- `entropy_threshold: 0.95` → 閾値が高すぎてTTTが発動しにくい

---

## 6. データセット固有の問題

### 6.1 HGDデータセットの特性

- **44チャンネル**: 情報量が多い
- **高品質信号**: ノイズが少ない
- **高いベースライン精度**: 92.95%

**TTTへの影響:**
```
高品質データ
    ↓
Baseモデルがほぼ最適解
    ↓
TTTが「ノイズ」として作用
    ↓
精度低下
```

### 6.2 BCIC 2bの特性

- **3チャンネルのみ**: 限られた情報
- **2クラス分類**: 比較的容易
- **訓練時間10.9倍短縮**: 効率面では成功

**TTTへの影響:**
- 少チャンネルではTTTの影響範囲が限定的
- 精度低下は-1.9%で許容範囲

---

## 7. 改善提案

### 7.1 短期的改善（即座に実装可能）

1. **訓練時にもEntropy Gatingを有効化**
```yaml
# 修正: 訓練とテストで同じ挙動にする
entropy_gating_in_train: True  # 現在はFalse
```

2. **TTT学習率の低減**
```python
ttt_lr = 0.01  # 現在
ttt_lr = 0.001 # 提案（10分の1）
```

3. **max_epochsの増加**
```yaml
max_epochs: 500  # 十分な学習を保証
```

4. **エントロピー閾値の調整**
```yaml
entropy_threshold: 0.7  # 現在は0.95（高すぎる可能性）
```

### 7.2 中期的改善

1. **条件付きTTT**
   - 予測が不確実な時のみTTTを適用
   - Baseの精度が高い被験者ではTTTを無効化

2. **データセット適応的ゲーティング**
   - データセットの特性を学習してα を調整
   - Meta-learningの導入

### 7.3 長期的研究方向

1. **Uncertainty-Aware TTT**
   - Aleatoric vs Epistemic uncertaintyの分離
   - Epistemic uncertaintyが高い時のみ適応

2. **Knowledge Distillation**
   - BaseモデルからHybridへ知識を蒸留
   - 安定性を維持しながら適応性を獲得

---

## 8. 結論

### うまくいかなかった主な理由

1. **訓練時とテスト時の挙動不一致** ⭐最重要
   - 訓練: 静的α（常にTTT）
   - テスト: 動的α（エントロピーベース）
   - モデルが学習した挙動とテスト時の挙動が異なる

2. **データセット特性との不適合**
   - 高品質データ（HGD）では不確実な予測が少ない
   - TTTの発動機会がほとんどなく、学習が不十分

3. **学習不足**
   - Early Stoppingによる打ち切り
   - TTTアダプターが十分に収束していない

4. **閾値設定の問題**
   - entropy_threshold=0.95は高すぎる可能性
   - TTTがほとんど発動しない状況

### 成果

- **訓練時間の大幅短縮**: 最大10.9倍
- **標準偏差の改善**: 2aで9.25→5.52（安定性向上）
- **一部被験者での改善**: S4@2b, S11@HGD

### 次のステップ

1. `entropy_gating_in_train: True` で訓練/テストの挙動を統一
2. `entropy_threshold: 0.7` に調整して再実験
3. TTT学習率を低減して再実験
4. max_epochsを増加して十分な学習を保証

---

## 付録: 被験者別完全データ

### A. BCIC IV 2a
| Subject | Base | Hybrid | Δ |
|---------|------|--------|---|
| 1 | 91.4% | 81.0% | -10.4% |
| 2 | 63.8% | 75.9% | **+12.1%** |
| 3 | 89.7% | 87.9% | -1.7% |
| 4 | 84.5% | 79.3% | -5.2% |
| 5 | 74.1% | 79.3% | **+5.2%** |
| 6 | 86.2% | 81.0% | -5.2% |
| 7 | 94.8% | 94.8% | 0.0% |
| 8 | 86.2% | 87.9% | **+1.7%** |
| 9 | 91.4% | 84.5% | -6.9% |

### B. BCIC IV 2b
| Subject | Base | Hybrid | Δ |
|---------|------|--------|---|
| 1 | 82.5% | 77.5% | -5.0% |
| 2 | 81.3% | 80.0% | -1.3% |
| 3 | 73.8% | 75.0% | **+1.3%** |
| 4 | 98.8% | 100.0% | **+1.2%** |
| 5 | 79.8% | 81.0% | **+1.2%** |
| 6 | 87.5% | 82.5% | -5.0% |
| 7 | 80.0% | 80.0% | 0.0% |
| 8 | 83.0% | 78.4% | -4.5% |
| 9 | 77.5% | 72.5% | -5.0% |

### C. HGD
| Subject | Base | Hybrid | Δ |
|---------|------|--------|---|
| 1 | 90.0% | 77.5% | -12.5% |
| 2 | 83.8% | 64.4% | -19.4% |
| 3 | 97.5% | 92.5% | -5.0% |
| 4 | 100.0% | 98.1% | -1.9% |
| 5 | 100.0% | 75.0% | **-25.0%** |
| 6 | 95.6% | 80.6% | -15.0% |
| 7 | 86.9% | 51.3% | **-35.6%** |
| 8 | 96.3% | 68.1% | **-28.1%** |
| 9 | 100.0% | 99.4% | -0.6% |
| 10 | 95.0% | 87.5% | -7.5% |
| 11 | 92.5% | 93.1% | **+0.6%** |
| 12 | 98.8% | 93.1% | -5.6% |
| 13 | 90.0% | 68.1% | -21.9% |
| 14 | 75.0% | 61.3% | -13.8% |

