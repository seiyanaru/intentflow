# general
dataset_name: "" 
subject_ids: "all"
max_epochs: 1000
max_epochs_2b: 50
max_epochs_loso: 50
max_epochs_loso_hgd: 50
seed: 42

# preprocessing
z_scale: True
interaug: True
data_path: "/workspace-cloud/seiya.narukawa/intentflow/data/raw/BCICIV_2a_gdf/"
data_path_2b: "/workspace-cloud/seiya.narukawa/intentflow/data/raw/BCICIV_2b_gdf/"

preprocessing:
  bcic2a:
    sfreq: 250
    low_cut: null
    high_cut: null
    start: 0.0
    stop: 4.0
    batch_size: 48
  bcic2b:
    sfreq: 250
    low_cut: null
    high_cut: null
    start: 0.0
    stop: 3.0  # Fixed 3.0 seconds (750 samples @ 250Hz)
    batch_size: 48
  bcic3:
    sfreq: 100
    low_cut: 0
    high_cut: 40
    start: 0.0
    stop: 0.0
    batch_size: 28
    channel_selection: True
  reh_mi:
    sfreq: 128
    low_cut: 4
    high_cut: 50
    start: 0.0
    stop: 0.0
    batch_size: 48
  hgd:
    sfreq: 250
    low_cut: 4
    high_cut: null
    start: 0.0
    stop: 0.0
    batch_size: 48
    remove_artifacts: True
  bcic4_2a:
    sfreq: 250
    low_cut: null
    high_cut: null
    start: 0.0
    stop: 4.0
    batch_size: 48

# model
model: "TCFormer_Hybrid"

model_kwargs:
  F1: 32
  temp_kernel_lengths: [20, 32, 64]
  d_group: 16
  D: 2
  pool_length_1: 8
  pool_length_2: 7
  dropout_conv: 0.4
  use_group_attn: True

  # Transformer parameters (Shared with TTT)
  q_heads : 4
  kv_heads : 4
  trans_depth: 2
  trans_dropout: 0.4
  
  # Optimization parameters
  lr: 0.001
  weight_decay: 0.05
  patience: 10
  min_epochs: 20
  
  # TTT Specific Configuration
  ttt_config:
    layer_type: "linear"
    # Lower base_lr to avoid overly aggressive adaptation.
    base_lr: 0.1
    mini_batch_size: 16
    share_qk: false
    use_dual_form: true
    learnable_init_state: false
    # Stronger anchoring to prevent drift on hard subjects (S2) and protect stable ones (S1/S5).
    ttt_reg_lambda: 0.05
    # Do not scale anchoring by lr_scale (keeps stability when entropy is high).
    ttt_anchor_scale_mode: "none"
    # Scale inner-loop gradients to avoid "always clipped" saturation.
    ttt_loss_scale: 0.1
    ttt_grad_clip: 1.0     # Gradient clipping threshold for TTT updates

  # Hybrid Learning Configuration (Optimal parameters from grid search)
  hybrid_config:
    lr: 0.1              # Learning rate for hybrid learning
    reg: 0.05            # L2 regularization coefficient
    ratio: 0.25          # Ratio of data used for hybrid learning (25%)
  
  # Dynamic Gating Configuration
  use_dynamic_gating: True  # Toggle for input-dependent gating mechanism
  gating_mode: "entropy"    # "feature_stats" | "entropy"
  # Entropy-driven reactive adaptation: alpha/lr_scale = sigmoid(w * H + b)
  entropy_alpha_init_w: 2.0
  entropy_alpha_init_b: -3.0
  entropy_lr_init_w: 2.0
  entropy_lr_init_b: -2.0
  entropy_gating_in_train: False   # run 2-pass entropy gating only in eval by default
  # Dead-zone + caps (protect S1/S5 from unnecessary adaptation)
  entropy_threshold: 0.95
  alpha_max: 0.5
  lr_scale_max: 0.5

  # Optional: encourage group-attn to be selective (minimize entropy of group weights)
  # Set to 0.0 to disable.
  group_attn_entropy_reg: 0.0

save_checkpoint: True
