# TCFormer_Hybrid モデル詳細

## 概要

`TCFormer_Hybrid` は、EEG信号の分類タスクにおいて、従来の **TCFormer** (Temporal Convolutional Transformer) の強力な特徴抽出能力と、**Test-Time Training (TTT)** による動的適応能力を融合させたモデルです。

最大の特徴は、**固定されたSelf-Attention（普遍的な特徴表現）** と **適応的なTTTアダプター（個人差への適応）** を**並列**に配置した「ハイブリッドエンコーダ」構造にあります。これにより、安定した性能を維持しつつ、テストデータ（被験者ごとの分布）に柔軟に適応することが可能です。

## アーキテクチャ詳細

モデル全体のデータフローは以下の通りです：
`Input (EEG) -> [Multi-Kernel Conv Block] -> [Hybrid Encoder] -> [TCN Head] -> Output (Class Prob)`

各コンポーネント、特に **Hybrid Encoder** の内部ロジックについて詳述します。

### 1. Multi-Kernel Conv Block (特徴抽出)
*   **入力**: 生のEEG信号 $X \in \mathbb{R}^{B \times C \times T}$ ($B$:バッチサイズ, $C$:チャンネル数, $T$:時間長)
*   **処理**: 複数の異なるカーネルサイズ ($K_1, K_2, ...$) を持つ1D畳み込み層を並列に適用し、出力を結合します。
*   **出力**: 特徴マップ $H_{conv} \in \mathbb{R}^{B \times T' \times D_{model}}$
    *   時間次元 $T$ はプーリングにより縮小され、チャンネル次元はモデル次元 $D_{model}$ に拡張されます。

### 2. Hybrid Encoder (特徴変換・適応)
このブロックが本モデルの核心です。入力特徴 $H$ に対して、以下の計算を行います。

#### A. 並列処理構造 (Parallel Hybrid Block)
通常のTransformerブロックが $H + \text{Attention}(H)$ を計算するのに対し、Hybridブロックは以下のように計算します：

$$ H' = H + \text{Attention}(\text{Norm}_1(H)) + \alpha \cdot \text{TTT\_Adapter}(\text{Norm}_1(H)) $$
$$ H_{out} = H' + \text{MLP}(\text{Norm}_2(H')) $$

*   **Main Path: Self-Attention (Fixed)**
    *   事前学習済みの重み $W_Q, W_K, W_V$ を使用し、テスト時には**固定**されます。
    *   入力シーケンス全体の広域的な依存関係（Global Context）を捉え、被験者に共通する普遍的な特徴を抽出します。
    
*   **Adapter Path: TTT Adapter (Adaptive)**
    *   テストデータを用いて、その場で重みを更新しながら変換を行うパスです。

*   **統合と $\alpha$ (Gating Factor)**
    *   Attention出力とAdapter出力は、同じ次元 $D_{model}$ を持ち、**要素ごとの加算 (Element-wise Sum)** によって統合されます。
    *   $\alpha$ (`adapter_scale`) は、TTT Adapterの影響度を制御する**学習可能なスカラーパラメータ (Learnable Parameter)** です。
        *   **初期化**: $\alpha$ は **0.0** で初期化されます（Zero-Init戦略）。
        *   **最適化ロジック (勾配降下法)**:
            学習中、損失関数 $L$ を最小化するように $\alpha$ が更新されます。
            $$ \frac{\partial L}{\partial \alpha} = \frac{\partial L}{\partial H'} \cdot \frac{\partial H'}{\partial \alpha} = \frac{\partial L}{\partial H'} \cdot H_{TTT} $$
            *   $\frac{\partial L}{\partial H'}$: 最終的な分類精度を上げるために、出力 $H'$ がどう変化すべきかを示す誤差信号。
            *   $H_{TTT}$: TTTアダプターの出力。
            *   **判定基準**: もし $H_{TTT}$ が分類精度の向上（損失 $L$ の低下）に役立つ情報を含んでいれば、勾配により $\alpha$ は増加します。逆に、ノイズであれば $\alpha$ は増加せず、アダプターの影響は抑制されます。これが「モデルが必要と判断すれば」という挙動の数学的実体です。

#### B. TTT Adapter の内部ロジック
計算コストを抑えつつ適応能力を持たせるため、ボトルネック構造を採用しています。

1.  **次元圧縮 (Down-projection)**:
    *   入力次元 $D_{model}$ を $D_{adapter} = r \times D_{model}$ に圧縮します（$r$ は `adapter_ratio`、例: 0.25）。
    *   $X_{low} = \sigma(X W_{down})$

2.  **Test-Time Training (TTT) Layer**:
    *   圧縮された特徴 $X_{low}$ を用いて、内部状態（重み $W_{hidden}$）を更新しながら出力 $Y_{low}$ を計算します。
    *   **更新則 (Dual Form)**:
        $$ W_{t} = W_{t-1} - \eta \nabla \ell(W_{t-1}; x_t) $$
        ここで $\ell$ は自己教師あり損失（再構成誤差など）です。この更新により、モデルは「現在の入力データの分布」に適応します。
    *   **特徴変換**:
        $$ y_t = f(x_t; W_t) $$
        更新された重みを用いて特徴を変換するため、出力 $Y_{low}$ は入力データに適応した表現となります。

3.  **次元復元 (Up-projection)**:
    *   適応後の特徴 $Y_{low}$ を元の次元 $D_{model}$ に戻します。
    *   $Y_{out} = \text{Norm}(Y_{low}) W_{up}$
    *   ここで、Zero-Init戦略の一環として、$W_{up}$ はゼロで初期化されることが一般的で、これにより初期段階でのAdapter出力は完全にゼロになります。

### 3. TCN Head (分類)
*   **入力**: ハイブリッドエンコーダからの出力 $H_{out}$
*   **処理**: 時間方向の情報を集約するTemporal Convolutional Network (TCN)。
*   **出力**: クラス分類のロジット（確率）。

## 評価指標と可視化のロジック

本プロジェクトで生成される図表（`fig/` フォルダ内）の生成ロジックと見方についての解説です。

### 1. エントロピー分布 (Entropy Distribution)
**「AIがどれくらい自信を持って答えているか？」** を数値化して分布図にしたものです。

*   **計算ロジック**:
    1.  **ロジット (Logits)**: モデルの最終出力（確率になる前の生スコア）。
    2.  **確率への変換 (Softmax)**: 
        $$ p_i = \frac{e^{z_i}}{\sum e^{z_k}} $$
        （例: クラスAである確率 70%）
    3.  **エントロピー計算 (Shannon Entropy)**:
        $$ H = -\sum p_i \log(p_i) $$
        
*   **値の意味**:
    *   **低い ($H \approx 0$)**: 「自信満々」。特定のクラスの確率が非常に高い状態。
    *   **高い**: 「迷っている」。どのクラスの確率も似たり寄ったりな状態。

*   **グラフの見方**:
    *   **左側（0付近）に山がある**: モデルは迷いなく明確な判断を下しています（理想的）。
    *   **右側に広がっている**: モデルは判断に迷いが生じています（不確実性が高い）。
    *   HybridモデルがBaseモデルより左に寄っていれば、適応によって「迷いが減った」ことを意味します。

### 2. t-SNE (特徴量空間の可視化)
高次元の特徴ベクトルを2次元に圧縮し、モデルがデータをどのように「見て」いるかを可視化します。

*   **ロジック**: 分類ヘッド直前の特徴ベクトル（通常100次元以上）を入力とし、似た特徴を持つデータは近くに、異なるデータは遠くになるように配置します。
*   **見方**:
    *   **きれいな塊（クラスタ）ができている**: クラス分離がうまくいっています。
    *   **色が混ざり合っている**: モデルがクラスの違いを識別できていません。
    *   **Hybrid vs Base**: Hybridでクラスタがより凝集していれば、適応成功です。

## なぜこの構造が有効なのか？ (Design Rationale)

1.  **安定性と可塑性のバランス (Stability-Plasticity Balance)**:
    *   **Attentionパス (安定性)**: 学習データ全体から得られた知識を保持し、未知のデータに対しても最低限の性能を保証します（破滅的忘却の防止）。
    *   **TTTパス (可塑性)**: テスト時のドメインシフト（被験者ごとの脳波パターンの違いなど）に対して、パラメータを動的に変化させて対応します。

2.  **Zero-Initによる学習の安定化**:
    *   $\alpha=0$ から開始することで、初期学習段階でのTTTによる不安定な挙動（まだうまく適応できない状態）がメインパスの学習を阻害するのを防ぎます。

3.  **効率的な適応**:
    *   巨大なSelf-Attention層全体を適応させるのではなく、軽量なAdapter部分のみを適応させることで、計算コストとメモリ使用量を大幅に削減しています。

## 設定パラメータ

`configs/tcformer_hybrid/tcformer_hybrid.yaml` にて設定可能です。

```yaml
model_kwargs:
  # ...
  adapter_ratio: 0.25  # アダプターの圧縮率。小さいほど軽量。
  
  ttt_config:
    base_lr: 0.1       # TTT層内部の学習率（適応の速さ）
    ttt_reg_lambda: 0.01 # 正則化係数（初期重みからの乖離を抑制）
```

## 想定される疑問点 (Q&A)

### Q1: なぜTTTモデル単体ではなく、Hybrid構造にする必要があるのですか？
**A:** EEGのようなノイズが多く個人差の大きいデータでは、TTTモデル単体だとテストデータへの**過剰適合（Overfitting）**や、事前学習で得た知識の**破滅的忘却（Catastrophic Forgetting）**が起きやすいためです。
Hybrid構造にすることで、Self-Attentionパスが「変わらない普遍的な特徴」を維持し、TTTパスが「個人差や変動」のみを補正するという役割分担ができ、安定性と適応性の両立が可能になります。

### Q2: $\alpha$ (adapter_scale) が学習中に負の値になることはありますか？
**A:** 数学的にはあり得ますが、実際の学習では通常 **正の値** に収束します。これは、TTTによる適応特徴 $H_{TTT}$ が、元の特徴 $H$ と相関が高く、かつ分類に有用な情報を含んでいるためです。
もし負になった場合は、TTTパスの出力が「逆効果」になっていることを示唆しており、モデルが自動的にその影響を反転・抑制しようとしていると解釈できます。

### Q3: `adapter_ratio` を大きくすればするほど性能は向上しますか？
**A:** 必ずしもそうとは限りません。`adapter_ratio` を大きくすると表現力は増しますが、同時に計算コストが増大し、学習パラメータ数が増えることで過学習のリスクも高まります。
実験的には、`0.25` 程度（元の次元の1/4）が計算効率と精度のバランスが良い**スイートスポット**であることが多いです。これは、LoRA（Low-Rank Adaptation）などの研究知見とも整合します。

### Q4: 推論速度（レイテンシ）への影響はどの程度ですか？
**A:** TTTの計算は追加コストになりますが、本モデルでは以下の工夫により影響を最小限に抑えています。
1.  **並列処理**: Self-AttentionとTTT AdapterはGPU上で並列に計算可能です。
2.  **ボトルネック構造**: `adapter_ratio` による次元圧縮により、TTTの計算量はSelf-Attention本体よりも遥かに小さくなっています。
実測でも、純粋なTCFormerと比較して数ミリ秒程度の増加に収まる設計となっています。

### Q5: Subject 2 (S2) で精度が向上し、Subject 1 (S1) で精度が低下したのはなぜですか？
**A:** これは**「適応の利得」と「適応のコスト」のトレードオフ**によるものです。
*   **S2（精度向上）**: S2は学習データとテストデータの分布のズレ（ドメインシフト）が大きい「難しい被験者」です。固定モデルだけでは対応できないズレを、**TTTアダプターが動的に補正**したため、精度が大幅に向上しました。
*   **S1（精度低下）**: S1は分布が安定しており、固定モデルだけで既に最適な状態でした。そこにTTTによる適応を加えた結果、**分類に不要な微細な変動（ノイズ）まで学習**してしまい、かえって特徴量を最適解から遠ざけてしまった（Over-adaptation）と考えられます。
これは「全ての被験者に一律に適応すべきではなく、分布のズレが大きい時のみ強く適応すべき」という今後の課題を示唆しています。
