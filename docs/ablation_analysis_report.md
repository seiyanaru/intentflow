# アブレーション実験 詳細分析レポート

## 1. 実験概要

### 1.1 実験設定

| 実験 | 設定 | 目的 |
|------|------|------|
| **Exp A** | `gating_mode=feature_stats` | エントロピー経路が原因かを確認 |
| **Exp B** | `gating_mode=entropy`, `threshold=0.7`, `entropy_gating_in_train=false` | 閾値調整で2bも発火するか確認 |
| **Exp C** | `gating_mode=entropy`, `threshold=0.7`, `entropy_gating_in_train=true` | Train/Test不一致がHGD崩壊の原因か確認 |
| **Exp D** | Exp C + `ttt_drop_prob=0.2` | "TTTが無いと死ぬモデル"を防げるか確認 |

### 1.2 結果サマリー

| 実験 | BCIC 2a | BCIC 2b | HGD | HGD Δ from Base |
|------|---------|---------|-----|-----------------|
| **Base** | 84.68% | 82.69% | **92.95%** | - |
| **Exp A** | 83.91% | 80.76% | 78.75% | **-14.20%** |
| **Exp B** | 83.91% | 80.76% | 78.75% | **-14.20%** |
| **Exp C** | 82.76% | 81.44% | 79.73% | **-13.22%** |
| **Exp D** | 81.03% | **82.16%** | 79.69% | **-13.26%** |

---

## 2. 分析結果

### 2.1 🔍 主要発見: エントロピーと精度低下の相関

**HGDでの相関分析結果:**
```
Entropy vs Accuracy Drop: r = 0.6655 (強い正の相関)
```

**意味**: エントロピー（予測の不確実性）が高い被験者ほど、精度低下が大きい。

| 被験者グループ | エントロピー | 精度低下 | 分析 |
|---------------|------------|---------|------|
| S3, S5 (安定) | < 0.65 | < 5% | 低エントロピー → TTT適応が控えめ → 安定 |
| S6, S7, S8, S10, S11, S13 (崩壊) | > 0.70 | > 15% | 高エントロピー → TTT過剰適応 → 精度崩壊 |

### 2.2 被験者ごとの詳細分析（HGD, Exp C）

```
Subject    Acc      Alpha    Entropy   LR Scale   Analysis
---------------------------------------------------------------
S5         99.4%    0.0131   0.5889    0.0261     ✅ 低エントロピー → 安定
S3         93.1%    0.0218   0.6251    0.0457     ✅ 低エントロピー → 安定
S7         63.1%    0.0580   0.7436    0.1200     ⚠️ 高エントロピー → 過適応
S11        71.9%    0.0560   0.7200    0.1127     ⚠️ 高エントロピー → 過適応
```

**重要な観察:**
- S5は**エントロピー0.59**と低く、TTT学習率は**0.026**（最小）→ 99.4%維持
- S7は**エントロピー0.74**と高く、TTT学習率は**0.120**（約4.6倍）→ 63.1%に崩壊

---

## 3. 根本原因の特定

### 3.1 ⚠️ 原因1: TTTの過剰適応（最重要）

**メカニズム:**
```
高エントロピー → 高α（TTT寄与率）+ 高LR → TTTが強く適応 → ノイズを学習 → 精度低下
```

**データセット間比較（同一S7）:**
| Dataset | Alpha | Entropy | LR Scale | 結果 |
|---------|-------|---------|----------|------|
| BCIC 2a | 0.0196 | 0.6607 | 0.0451 | 96.55%（良好） |
| HGD | 0.0580 | 0.7436 | 0.1200 | 63.13%（崩壊） |

HGDのS7はBCIC 2aのS7と比較して:
- **Alpha: 約3倍**
- **LR Scale: 約2.7倍**

→ HGDで予測が不確実な被験者に対して、TTTがより強く適応しようとし、過適応が発生。

### 3.2 ⚠️ 原因2: データセット特性の違い

| 特性 | BCIC 2a/2b | HGD |
|------|-----------|-----|
| クラス数 | 4 | **2** |
| 最大エントロピー | ln(4) ≈ 1.39 | **ln(2) ≈ 0.69** |
| 正規化エントロピー範囲 | 広い | **狭い** |
| 被験者数 | 9 | **14** |
| Base精度 | 82-85% | **92.95%** |
| 被験者間変動 | 小 | **大（75%〜100%）** |

**問題点:**
1. **2クラス問題の特性**: エントロピーの絶対値が小さい（最大0.69）ため、閾値0.7では多くの被験者でTTTが発動
2. **Baseの高精度**: 92.95%という高い精度を持つBaseに対して、TTTは「改善の余地」がほとんどない
3. **被験者間変動**: 75%〜100%と大きな変動があり、「難しい被験者」でTTTが過剰に発動

### 3.3 ⚠️ 原因3: Exp A/Bの結果が同一である理由

**Exp A (feature_stats) = Exp B (entropy) の結果が同一**

これは非常に重要な発見で、以下を示唆：

1. **Eval時のゲーティング方式は影響が小さい**
   - feature_stats と entropy の両方で同じ結果
   - → 問題は「どのゲーティングを使うか」ではない

2. **根本的な問題は「TTT層自体」にある**
   - ゲーティングをどう変えても、TTT層がテスト時に更新される限り問題は解決しない
   - TTTの内部更新（重み変更）が精度低下の主因

---

## 4. なぜ各実験で改善しなかったか

### 4.1 Exp A/B: ゲーティング方式の変更

**期待**: エントロピーゲーティングを無効化すれば改善する

**結果**: 改善なし（-14.20%）

**理由**: 
- 問題は「ゲーティング」ではなく「TTT層の更新」自体
- feature_statsゲーティングでもTTTは発動し、過適応を引き起こす

### 4.2 Exp C: Train/Test不一致の解消

**期待**: 訓練時もエントロピーゲーティングを使えば一貫性が生まれる

**結果**: わずかに改善（-13.22%）

**理由**: 
- 訓練時と推論時の一貫性は向上
- しかし、「高エントロピー被験者への過剰適応」は解決されない
- むしろ、訓練時からエントロピーに基づくゲーティングを学習するため、過適応が「学習された行動」になる

### 4.3 Exp D: TTT Dropの追加

**期待**: 訓練中にTTTを確率的にドロップすれば、Attention-only経路も強化される

**結果**: BCIC 2bで最良（82.16%）、HGDでは改善なし

**理由**: 
- BCIC 2bでは効果的（TTTが無くても機能するモデルを学習）
- HGDでは、20%のドロップ確率では不十分
- HGDの「難しい被験者」では、テスト時にTTTが依然として強く発動

---

## 5. 結論と洞察

### 5.1 HGD崩壊の根本原因

```
┌─────────────────────────────────────────────────────────────────┐
│  HGD崩壊の因果関係                                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. Baseモデルが既に高精度 (92.95%)                             │
│      ↓                                                          │
│  2. 「改善の余地」が少ない状態でTTTが発動                       │
│      ↓                                                          │
│  3. 予測が不確実な被験者 (高エントロピー) で特にTTTが強く発動   │
│      ↓                                                          │
│  4. TTTがテストデータのノイズを学習 (過適応)                    │
│      ↓                                                          │
│  5. 精度が大幅に低下 (特にS7: -23.7%, S8: -21.9%, etc.)         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 5.2 重要な教訓

1. **TTTは「改善の余地がある」ケースで効果的**
   - Baseが低精度のデータセットでは有効かもしれない
   - 既に高精度のデータセットでは逆効果

2. **エントロピーゲーティングの罠**
   - 高エントロピー = 予測が不確実 = 「難しいサンプル」
   - 難しいサンプルでTTTを強化 → 過適応のリスク増大

3. **2クラス vs 4クラス**
   - 2クラス問題ではエントロピーの範囲が狭い
   - 閾値ベースのゲーティングが機能しにくい

### 5.3 推奨される次のステップ

1. **HGDではTTTを完全に無効化** (`alpha=0`固定)
2. **エントロピーゲーティングの逆転**: 高エントロピーでTTTを抑制、低エントロピーで有効化
3. **データセット別の閾値最適化**: 2クラス用の低い閾値
4. **TTT学習率の大幅削減**: 0.1 → 0.01 またはそれ以下

---

## 6. 補足: 数値データ

### 6.1 HGD全被験者の詳細（Exp C）

| Subject | Base Acc | Exp C Acc | Δ | Alpha | Entropy | LR Scale |
|---------|----------|-----------|---|-------|---------|----------|
| S1 | 90.0% | 75.6% | -14.4% | 0.0277 | 0.6973 | 0.0597 |
| S2 | 83.8% | 78.1% | -5.6% | 0.0373 | 0.7289 | 0.0769 |
| S3 | 97.5% | 93.1% | -4.4% | 0.0218 | 0.6251 | 0.0457 |
| S4 | 100.0% | 88.1% | -11.9% | 0.0320 | 0.6238 | 0.0614 |
| S5 | 100.0% | 99.4% | -0.6% | 0.0131 | 0.5889 | 0.0261 |
| S6 | 95.6% | 76.9% | -18.8% | 0.0499 | 0.7464 | 0.0994 |
| S7 | 86.9% | 63.1% | -23.7% | 0.0580 | 0.7436 | 0.1200 |
| S8 | 96.2% | 74.4% | -21.9% | 0.0429 | 0.7270 | 0.0939 |
| S9 | 100.0% | 90.0% | -10.0% | 0.0378 | 0.6663 | 0.0712 |
| S10 | 95.0% | 76.9% | -18.1% | 0.0461 | 0.7203 | 0.0930 |
| S11 | 92.5% | 71.9% | -20.6% | 0.0560 | 0.7200 | 0.1127 |
| S12 | 98.8% | 85.6% | -13.1% | 0.0497 | 0.7010 | 0.0986 |
| S13 | 90.0% | 74.4% | -15.6% | 0.0417 | 0.7364 | 0.0860 |
| S14 | 75.0% | 68.8% | -6.2% | 0.0893 | 0.7306 | 0.1416 |

**相関係数**: Entropy vs Accuracy Drop = **0.6655**

