# general
dataset_name: "" 
subject_ids: "all"
max_epochs: 1000
max_epochs_2b: 50
max_epochs_loso: 50
max_epochs_loso_hgd: 50
seed: 42

# preprocessing
z_scale: True
interaug: True
data_path: "/workspace-cloud/seiya.narukawa/intentflow/data/raw/BCICIV_2a_gdf/"

preprocessing:
  bcic2a:
    sfreq: 250
    low_cut: null
    high_cut: null
    start: 0.0
    stop: 4.0
    batch_size: 48
  bcic2b:
    sfreq: 250
    low_cut: null
    high_cut: null
    start: 0.0
    stop: -0.5
    batch_size: 48
  bcic3:
    sfreq: 100
    low_cut: 0
    high_cut: 40
    start: 0.0
    stop: 0.0
    batch_size: 28
    channel_selection: True
  reh_mi:
    sfreq: 128
    low_cut: 4
    high_cut: 50
    start: 0.0
    stop: 0.0
    batch_size: 48
  hgd:
    sfreq: 250
    low_cut: 4
    high_cut: null
    start: 0.0
    stop: 0.0
    batch_size: 48
    remove_artifacts: True
  bcic4_2a:
    sfreq: 250
    low_cut: null
    high_cut: null
    start: 0.0
    stop: 4.0
    batch_size: 48

# model
model: "TCFormer_Hybrid"

model_kwargs:
  F1: 32
  temp_kernel_lengths: [20, 32, 64]
  d_group: 16
  D: 2
  pool_length_1: 8
  pool_length_2: 7
  dropout_conv: 0.4
  use_group_attn: True

  # Transformer parameters (Shared with TTT)
  q_heads : 4
  kv_heads : 4
  trans_depth: 2
  trans_dropout: 0.4
  
  # Optimization parameters
  lr: 0.001
  weight_decay: 0.05
  patience: 10
  min_epochs: 20
  
  # TTT Adapter Specific Configuration
  ttt_config:
    layer_type: "linear"
    base_lr: 0.1 # Adaptive path learning rate
    mini_batch_size: 16
    share_qk: false
    use_dual_form: true
    learnable_init_state: false
    ttt_reg_lambda: 0.01 # Regularization

save_checkpoint: True
